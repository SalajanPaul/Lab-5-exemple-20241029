{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors este un algoritm pentru învățarea supravegheată. În cazul în care datele sunt „antrenate” cu puncte de date corespunzătoare clasificării lor. Odată ce un punct trebuie să fie prezis, acesta ia în considerare cele „K” puncte cele mai apropiate de acesta pentru a determina clasificarea sa. Algoritmul k-nearest neighbors (KNN) este un algoritm neparametric de învățare automată care poate fi utilizat atât pentru sarcini de clasificare, cât și de regresie. Acesta funcționează prin găsirea celor mai asemănătoare k instanțe din setul de instruire cu o nouă instanță și apoi predicția etichetei noii instanțe pe baza etichetelor celor mai apropiați k vecini. \n",
    "\n",
    "în scikit-learn, clasa KNeighborsClassifier implementează algoritmul celor mai apropiați k vecini pentru sarcini de clasificare. Următorul cod arată cum să construim un clasificator KNN cu 5 vecini:\n",
    "\n",
    "Parametrul n_neighbors specifică numărul de vecini care urmează să fie utilizați pentru predicție. Valoarea implicită este 5.\n",
    "    Alți parametri care pot fi reglați pentru clasificatorul KNN includ:\n",
    "    metric: Metrica distanței care urmează să fie utilizată pentru calcularea distanței dintre vecini. Metrica implicită este distanța euclidiană.\n",
    "\n",
    "    weights (ponderi) : Funcția de ponderare care urmează să fie utilizată pentru predicție. Funcția de ponderare implicită este uniformă, ceea ce înseamnă că toți vecinii sunt ponderați în mod egal.\n",
    "\n",
    "    algoritm: Algoritmul de utilizat pentru găsirea celor mai apropiați vecini. Algoritmul implicit este căutarea prin forță brută.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the iris data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes este un algoritm de învățare supravegheată care este utilizat pentru sarcini de clasificare. Acesta se bazează pe teorema Bayes, care afirmă că probabilitatea apariției evenimentului A, având în vedere că evenimentul B a avut deja loc, este egală cu probabilitatea apariției evenimentului A înmulțită cu probabilitatea apariției evenimentului B, având în vedere că evenimentul A a avut deja loc, împărțită la probabilitatea apariției evenimentului B.\n",
    "\n",
    "În scikit-learn, există trei implementări diferite ale clasificatorului Naive Bayes:\n",
    "    GaussianNB : Acest clasificator este utilizat pentru date care sunt distribuite normal.\n",
    "    MultinomialNB : Acest clasificator este utilizat pentru date care sunt numărate ca număr de apariții.\n",
    "    BernoulliNB : Acest clasificator este utilizat pentru date care sunt binare (0 sau 1).\n",
    "Iată câțiva dintre parametrii care pot fi reglați pentru clasificatorul Naive Bayes:\n",
    "    alpha: Parametrul de netezire. Acest parametru controlează gradul de netezire aplicat probabilităților estimate.\n",
    "\n",
    "    fit_prior: Dacă se potrivesc probabilitățile anterioare pentru clase. Dacă acest parametru este setat la False, atunci probabilitățile anterioare vor fi considerate egale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25)\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasa DecisionTreeClassifier din scikit-learn este un algoritm de învățare supravegheată care poate fi utilizat pentru sarcini de clasificare. Acesta funcționează prin construirea unui arbore decizional, care este o structură asemănătoare unui organigrama ce asociază caracteristicile cu etichetele.\n",
    "\n",
    "Clasa DecisionTreeClassifier are următorii parametri:\n",
    "\n",
    "criterion: Criteriul de divizare care trebuie utilizat. Criteriul implicit este „gini”, care minimizează impuritatea Gini. Alte criterii posibile includ „entropy” și „crossentropy”.\n",
    "\n",
    "max_depth: Adâncimea maximă a arborelui.\n",
    "\n",
    "min_samples_split: Numărul minim de eșantioane necesare pentru a diviza un nod.\n",
    "\n",
    "min_samples_leaf: Numărul minim de eșantioane necesare pentru a fi la un nod frunză.\n",
    "\n",
    "min_impurity_decrease: Scăderea minimă a impurității necesară pentru a diviza un nod.\n",
    "\n",
    "random_state: seed aleatorie utilizată pentru construcția arborelui.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
